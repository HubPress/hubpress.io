:hp-tags: linux, mesos, marathon, openam, forgerock
= Deploying openAM 12 on mesos

## Background

OpenAM provides open source Authentication, Authorization, Entitlement and Federation software. We chose it as part of our internal SSO platform.
The documentation of openAM goes into a lot of details about the installation that I don't need to go through here again. +
openAM 12 was not deigned as a stateless app, and is not meant to be runnig on an elastic env like mesos.

The purpose of this doc is to highlight the main pain points and challenges in deploying openAM on a mesos cluster

.*References*
* https://backstage.forgerock.com/#!/docs/openam/12.0.0/
* http://azlabs.blogspot.fr/2015/08/openam-12-sessions.html
* http://blogs.forgerock.org/petermajor/2015/08/sessions/

.*Constraints*
* Mesos cluster only runs stateless non-persistent apps
* Marathon framework to manage long running apps on Mesos
* Need to run multiple instances for HA

## Setup

.External config store
Configure openAM to use openDJ for storing its configuration. +
This is the first ste
   
We start with the marathon JSON to launch openAM on Mesos, then afterwards i will go throug
[source,json]
{
  "id": "openam",
  "cmd":"bash -x startup.sh",
  "cpus": 2,
  "mem": 1024,
  "env": {
    "OPENDJ_SERVER":"opendj.example.com",
    "OPENDJ_USER":"cn=admin",
    "OPENDJ_PASS":"password",
    "OPENAM_SUFFIX":"dc=openam,dc=example,dc=com",
    "JAVA_HOME": "/usr/lib/jvm/jre-1.8.0/bin"
  },
  "instances": 2,
  "ports": [
    31080
  ],
  "requirePorts": true,
  "constraints":[
    ["hostname","LIKE","slave0[1,2]"]
  ],
  "healthChecks": [{
    "path": "/auth/isAlive.jsp",
    "protocol": "HTTP",
    "portIndex": 0,
    "gracePeriodSeconds": 300,
    "intervalSeconds": 60,
    "timeoutSeconds": 20,
    "maxConsecutiveFailures": 3,
    "ignoreHttp1xx": false
  }],
  "uris": [
    "http://fileserver.example.com/openam-configurator-12.0.0.zip",
    "http://fileserver.example.com/jetty.tar.gz",
    "http://fileserver.example.com/openam.war",
    "http://fileserver.example.com/startup.sh"
  ]
}



`startup.sh` is a bash script that start openam on jetty, waits for it to be fully started by looking at the log file for *Server:main: Started*, then it will launch the openam-configurator tool to configure openAM.

openam-configurator takes in a config file +
`openam.config`
```
# Server properties
SERVER_URL=http://${HOSTNAME}:${PORT0}
DEPLOYMENT_URI=/openam
BASE_DIR=/tmp/openam
locale=en_US
PLATFORM_LOCALE=en_US
AM_ENC_KEY='some_random_string'
ADMIN_PWD='a_good_pass'
AMLDAPUSERPASSWD='another_good_pass'
COOKIE_DOMAIN=.eample.com
ACCEPT_LICENSES=true

# External configuration data store
DATA_STORE=dirServer
DIRECTORY_SSL=SIMPLE
DIRECTORY_SERVER=${OPENDJ_SERVER}
DIRECTORY_PORT=${OPENDJ_PORT}
DIRECTORY_ADMIN_PORT=${OPENDJ_ADMIN_PORT}
ROOT_SUFFIX=$OPENAM_SUFFIX
DS_DIRMGRDN=${OPENDJ_USER}
DS_DIRMGRPASSWD=${OPENDJ_PASS}

# External User store
USERSTORE_TYPE=LDAPv3ForOpenDS
USERSTORE_SSL=SIMPLE
USERSTORE_HOST=${OPENDJ_SERVER}
USERSTORE_PORT=${OPENDJ_PORT}
USERSTORE_SUFFIX=ou=people,dc=example, dc=com
USERSTORE_MGRDN=${OPENDJ_USER}
USERSTORE_PASSWD=${OPENDJ_PASS}

# Site config
# TODO: make it dynamic
LB_SITE_NAME=example
LB_PRIMARY_URL=http://sso.example.com:80/openam
LB_SESSION_HA_SFO=true
```
This config will make openam not use its embedded database, but use the openDJ cluster this way openam is almost _stateless_. Now everytime Marathon (re)starts openam it will start it and configure it with openDJ as its confid backend. If openAM was configured before it will not attempt to reconfigure it.

After that successfully finishing the health check from the json file will become good.and the app is considered started. +

## Obstacles
Now that may not be so easy, openAM is not a stateless app. and here are a few reasons why it is difficult

  * openAM does not remove dead hosts from its "cluster" and cleanup is not trivial as seen in this blog post http
 ://azlabs.blogspot.fr/2015/11/ssoadm-cli-for-scaling-and-de-scaling.html
 * if you have read the blog posts above about openAM sessions, you'd notice that sessions are owned/associated with the host (openAM) that created it
 

### Hosts

In order to eliminate the issues with servers in the cluster I fixed 2 slaves, and a single port for openam, this way there is always 2 hosts showing up in the "cluster", eliminating the need for cleanup. 
```
"ports": [
    31080
],
"requirePorts": true,

```
This part will tell Marathon to always assign PORT 31080 to openAM 
```
"constraints":[
  ["hostname","LIKE","slave0[1,2]"]
],
```
This line will instruct Marathon to only launch on slaves 1 and 2 +
This way the servers in openAM cluster are always slave1:31080 and slave2:31080 and we're done with of the points

### Clustering

Sessions are normally stored in the app server's memory, in my case it jetty. then if a user has a session started on one of the servers in the cluster, then that session is not available for other servers in the cluster + 
There are 2 mechanisms that openAM uses to share sessions in a cluster, first is the *"Cross Talk"* which is basically where servers can communicate among them to ask about who is the owner of a certain session and delegate managing it to that server. +
The other is *CTS* "Core Token Service" which provides a persistent and highly available token storage for OpenAM session, OAuth 2.0, SAML v2.0, and UMA tokens. +
This is basically openAM writing its sessions in openDJ which makes them available for all servers in the cluster.+
CTS can be configured either form the openAM console or using the openam-ssoadmtools


Now openAM is ready.

Next step is to configure your SSO solution
